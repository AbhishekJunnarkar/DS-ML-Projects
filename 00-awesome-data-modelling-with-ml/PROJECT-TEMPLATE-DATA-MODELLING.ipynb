{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786c9e6c-3c15-41a8-af79-c7f9d55146c3",
   "metadata": {},
   "source": [
    "# Project Objective: Template for all projects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e997bad-1ec0-43c1-b423-2b23b667c57d",
   "metadata": {},
   "source": [
    "## Module 1: DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb670b-606c-4a95-843d-836ea0c56b2d",
   "metadata": {},
   "source": [
    "#### Step 1.1 Import the required libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64ab1e-8da3-4631-99c0-4390efaaa46d",
   "metadata": {},
   "source": [
    "#### Step 1.2 Load the Data as a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8f1dc-48a2-41e3-b394-5666435b3a3c",
   "metadata": {},
   "source": [
    "### Step 2.1 Data Inspection - head(), info(), describe() etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3db6b-d6cc-4880-96e4-f7b18e93d5cd",
   "metadata": {},
   "source": [
    "## Module 2: DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c38bd-7d16-492e-b396-f0f996cf69d2",
   "metadata": {},
   "source": [
    "**Data Exploration**This is the process of describing, visualizing, and analyzing data to better understand it. It helps answer questions about the structure and nature of the data.\n",
    "\n",
    "**Instances and Features**: An instance (or record/observation) refers to a row of data, while a feature (or variable) refers to a column of data. Features can be categorical (discrete values) or continuous (infinite possible values).\n",
    "\n",
    "**Dimensionality and Sparsity**: Dimensionality refers to the number of features in a dataset, while sparsity and density describe the degree to which data exists for the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa05aaf-7f41-4a60-89b9-c4395db7792a",
   "metadata": {},
   "source": [
    "### Step 2.1 Describe to explore and understand a specific column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759fb39-6b91-4054-8008-cea44118b060",
   "metadata": {},
   "source": [
    "### Step 2.2 Data Aggregation - value_counts(), mean(), groupby(), sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc5240-03d1-4d19-85ca-89f1a7824ead",
   "metadata": {},
   "source": [
    "### Step 2.3 Data Cleaning - Handle missing values, duplicates, inconsistent formats, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4670de9-ac40-483c-9430-0de7b7766938",
   "metadata": {},
   "source": [
    "### Step 2.4 Data Storage - Save the cleaned data for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da121f-7d48-4dd0-8abd-41575db18d52",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3f435-7bce-4539-b5a0-0ac6083e71c8",
   "metadata": {},
   "source": [
    "## Module 3: DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea67b10-1b83-4c67-a34d-80ae62b45c27",
   "metadata": {},
   "source": [
    "### ** *Because certain data patterns are only understood when represented with a visualization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20ed9e0-8933-4519-87fc-c12d7eda9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command so that all the graphs or plots appear in our jupyter notebook just after the commands\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061eeddf-368f-4c98-a1e1-35bb3d56fad4",
   "metadata": {},
   "source": [
    "### Step 3.1 Comparision visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2a4d1-a992-4122-bdce-43a4f0f6d60b",
   "metadata": {},
   "source": [
    "#### Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf9f8a-891d-4e1e-a606-00412902d687",
   "metadata": {},
   "source": [
    "### Step 3.2 Relationship visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a2627-be97-40e3-b9be-44b798532c15",
   "metadata": {},
   "source": [
    "#### Scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52612f4c-f575-4d78-99c9-6dd659fbdbd2",
   "metadata": {},
   "source": [
    "### Step 3.3 Distribution visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c941e-296a-44d6-9c2d-71534d2cd33f",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d0750-0c29-4bb8-9ddd-70b5e8a00217",
   "metadata": {},
   "source": [
    "### Step 3.4 Composition visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024510e2-baea-4dba-98fc-ee28c793314d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcad59d-7dce-4e30-a312-6d2dd9a1a674",
   "metadata": {},
   "source": [
    "## Module 4: DATA MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6232ed4-9243-4719-8218-c37363e140c6",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "We will start the modelling with the decision trees.\n",
    "\n",
    "A **decision tree** is a machine learning approach that uses an inverted tree-like structure to model the relationship between independent variables and a dependent variable. \n",
    "\n",
    "It mimics human decision-making with **if-then-else rules**. Each decision node represents a question, and branches represent possible answers leading to further decision nodes or leaf nodes (outcomes). \n",
    "\n",
    "Decision trees are used for both **classification** (categorical outcomes) and **regression** (continuous outcomes) problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d360d90-fdf4-40ba-a1c9-ac9afef575c5",
   "metadata": {},
   "source": [
    "-  When Dependent variable is categorical or discrete value, such as TRUE/FALSE, YES/NO then we will build **classification tree**\n",
    "-  When Dependent variable is continious value like age, income, tempatature, we will build **Regression Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e6ec7-a282-45e9-92e4-56cf71907a44",
   "metadata": {},
   "source": [
    "### Step 4.1 Build Recursive Partitions to create child partitions that are purer than their parents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ee5fe-d35b-4385-b13d-eaa06329cb6c",
   "metadata": {},
   "source": [
    "### Recursive Partitioning\n",
    "Recursive partitioning is a process used to build classification trees by repeatedly splitting data into smaller subsets. The goal is to maximize the similarity (homogeneity) of items within each subset. Here's a simplified breakdown:\n",
    "\n",
    "- Initial Split: Start with the entire dataset and find the best way to split it into two subsets to maximize the similarity within each subset.\n",
    "- Subsequent Splits: For each subset, repeat the process of finding the best split to create even more homogeneous subsets.\n",
    "- Stopping Criteria: Continue this process until all instances in a subset are of the same class, all features are exhausted, or a user-defined condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c4cf2-0670-44ce-8dd9-036a695d77c5",
   "metadata": {},
   "source": [
    "### Step 4.2 Measure the Degree of Impurity within a partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc1564-c2e2-436f-b241-f6e1294d2be7",
   "metadata": {},
   "source": [
    "#### Step 4.2.1 Measure the Entropy\n",
    "- Entropy: A measure of impurity borrowed from information theory, representing the level of randomness or disorder within a partition. It is used by the C5.0 decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13354474-5df2-401e-8304-64d33e7ba7f3",
   "metadata": {},
   "source": [
    "#### Step 4.2.2 Measure the Gini Impurity\n",
    "- Gini Impurity: Another measure of impurity, representing the statistical dispersion within a partition. It is used by the CART decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903723b-fcf7-4043-ab27-13e8d7e0e85d",
   "metadata": {},
   "source": [
    "#### Step 4.2.3 Measure the Information Gain\n",
    "- Information Gain: The reduction in entropy that occurs as a result of a split, helping to determine the best split for the data.\n",
    "These concepts help classification tree algorithms decide where to split the data to create the most homogeneous partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6293a-b409-452b-b013-31df16e56118",
   "metadata": {},
   "source": [
    "###Â Step 4.2.4 Measure the Sum of Squared Residuals (SSR) \n",
    "- **Purpose**: A high SSR indicates high variability within the partition, meaning the values are very different from the mean. A low SSR indicates low variability, meaning the values are similar to the mean.\n",
    "- The **regression tree algorithm** evaluates possible splits by calculating the SSR for each partition. It chooses the split that results in the lowest combined SSR, thereby minimizing variability and improving the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e3579-b11a-4e94-a93d-bfac3b30cd81",
   "metadata": {},
   "source": [
    "### Step 4.2.5 Prune the Decision Tree\n",
    "- **Pre-pruning**: This involves setting criteria to limit the size of the tree during the recursive partitioning process. It helps prevent overfitting but might stop tree growth too early.\n",
    "- **Post-pruning**: This allows the tree to grow fully and then reduces its size afterward. It is more effective in discovering important patterns but is less efficient in terms of compute time.\n",
    "- **Cost Complexity Pruning**: This method balances the sum of squared residuals (SSR) with a complexity penalty to choose the best sub-tree. The complexity parameter (alpha) is tuned to find the optimal tree size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b487017-d7c4-485e-a322-6a4d2e958f01",
   "metadata": {},
   "source": [
    "### Step 4.2.5 Calculate the Tree Score\n",
    "- A tree score is a metric used to evaluate the quality of a decision tree, balancing its ability to explain the data with its complexity. It consists of two main components:\n",
    "\n",
    "- Sum of Squared Residuals (SSR): Measures how well the tree explains the data. Lower SSR indicates better fit.\n",
    "Tree Complexity Penalty: Accounts for the number of leaf nodes in the tree. It is the product of a user-defined complexity parameter (alpha) and the number of leaf nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb08f4-8f2f-4218-b0f7-3a287376a4c3",
   "metadata": {},
   "source": [
    "## Module 5: DATA OPTIMIZATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
