{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686f2be7-cfab-4f57-a8de-23e30f6c2b57",
   "metadata": {},
   "source": [
    "# Concepts of Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b026330c-4c2f-4791-bf90-4f4a67de6548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization is a core concept in Artificail intelligence (AI) and Machine Learning(ML), as it directly impacts how models learn, improve and make predicitons\n"
     ]
    }
   ],
   "source": [
    "print('Optimization is a core concept in Artificail intelligence (AI) and Machine Learning(ML), as it directly impacts how models learn, improve and make predicitons')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5b004-3987-4160-ac47-8e55dd146de4",
   "metadata": {},
   "source": [
    "# Breakdown of Key Optimization functions in AI/ML:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cfaf1-7b78-41a2-ad03-67b509111ce1",
   "metadata": {},
   "source": [
    "## 1. Objective Function (Loss/Cost Function)\n",
    "This function quantifies how well the model is performing.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Mean Squared Error (MSE) for regression.\n",
    "\n",
    "Cross-Entropy Loss for classification.\n",
    "\n",
    "The goal of optimization is to minimize (or sometimes maximize) this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b3fa4-0566-4dfe-9c5b-0bd06bb5d85b",
   "metadata": {},
   "source": [
    "## Code Example of Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e31a26-d2af-4d62-b9e1-cca4addaed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([1, 2, 3, 4])\n",
    "y = np.array([2, 4, 6, 8])\n",
    "\n",
    "# Simple linear prediction\n",
    "def predict(w, X):\n",
    "    return w * X\n",
    "\n",
    "# Mean Squared Error Loss\n",
    "def loss(w):\n",
    "    y_pred = predict(w, X)\n",
    "    return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "print(loss(1.5))  # Try different w values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaca70-8963-49e5-9641-3a347d92e371",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent\n",
    "\n",
    "Most common optimization algorithm.\n",
    "\n",
    "Iteratively adjusts model parameters to minimize the loss.\n",
    "\n",
    "Variants:\n",
    "\n",
    " - Batch Gradient Descent: Uses the entire dataset.\n",
    "\n",
    " - Stochastic Gradient Descent (SGD): Uses one sample at a time.\n",
    "\n",
    " - Mini-Batch Gradient Descent: Compromise between batch and stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9e42b-af68-45aa-a252-46e306f1d6c7",
   "metadata": {},
   "source": [
    "## Code Example of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fd2c1-3332-4330-98c8-5b8cd85bd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code Example goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a46c45-3b11-4640-a3e5-ada055aef02c",
   "metadata": {},
   "source": [
    "## 3. Learning Rate\n",
    "Controls the size of steps taken during optimization.\n",
    "\n",
    "Too high: may overshoot the minimum.\n",
    "\n",
    "Too low: may converge very slowly or get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62fee1-95ed-400b-a540-eda08bd89bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Example of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9d3550-0e76-4d73-b5de-c25d9b1b5afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate\n"
     ]
    }
   ],
   "source": [
    "print('Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9793e-6752-41c7-b593-4d9cc2794263",
   "metadata": {},
   "source": [
    "## 4. Advanced Optimization Algorithms\n",
    "Adam (Adaptive Moment Estimation): Combines RMSprop and momentum.\n",
    "\n",
    "RMSprop: Adapts learning rates using moving average of squared gradients.\n",
    "\n",
    "Adagrad, Adadelta, Nadam: Other adaptive learning rate methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03cf25-87b7-4969-b1b4-3e0847e7deff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1119cd03-fc2f-4f84-9250-fdbed000a503",
   "metadata": {},
   "source": [
    "## 5. Regularization\n",
    "Prevents overfitting by penalizing large weights.\n",
    "\n",
    "L1 (Lasso): Encourages sparsity.\n",
    "\n",
    "L2 (Ridge): Penalizes large weights smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5efcb5-a6a0-4f6e-880c-567d9242df6f",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "Involves tuning model parameters like learning rate, batch size, and number of layers.\n",
    "\n",
    "Techniques:\n",
    "\n",
    "Grid Search\n",
    "\n",
    "Random Search\n",
    "\n",
    "Bayesian Optimization\n",
    "\n",
    "Hyperband/Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05717def-2b84-4b63-85f0-7564395e4ebe",
   "metadata": {},
   "source": [
    "## 7. Convex vs Non-Convex Optimization\n",
    "Convex problems (like linear regression) have a single global minimum.\n",
    "\n",
    "Neural networks involve non-convex optimization with multiple local minima and saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d212db-9017-47dc-85eb-73ccf5e95a6f",
   "metadata": {},
   "source": [
    "## 8. Early Stopping\n",
    "A regularization technique.\n",
    "\n",
    "Stops training when validation performance stops improving to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289bac74-c5b2-482f-b0e7-2265ee08d38f",
   "metadata": {},
   "source": [
    "## . Optimization in Reinforcement Learning\n",
    "Focuses on maximizing cumulative rewards rather than minimizing a loss.\n",
    "\n",
    "Uses techniques like policy gradient, Q-learning, or value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f1654-52a3-49cb-b28f-3667e62770b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc308175-f258-4a9a-a9f6-657ba80da7ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc7452-cfc8-4e1a-9f15-4f2d532a5303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
