# Table of contents

----

## 1. Optimization Formulations

- What is Optimization
- 
- What does it mean to optimize a function?
  - To optimize a function means to find the input value(s) (e.g., x) where the function:
    - Reaches a maximum (peak)
    - Or a minimum (valley)

- What Is the Power Rule?
   If you have a function like:

   f(x) = xⁿ

   Then the derivative is:

   f′(x) = n·xⁿ⁻¹
   In simple words:

   Bring the exponent n down in front as a multiplier
   Then subtract 1 from the exponent

   Examples
    Function	Derivative (using power rule)
    f(x) = x²	f′(x) = 2x
    f(x) = x³	f′(x) = 3x²
    f(x) = 5x⁴	f′(x) = 20x³
    f(x) = −2x²	f′(x) = −4x
    f(x) = x	f′(x) = 1
    f(x) = constant (e.g., 7)	f′(x) = 0

- Single variable objective function 
- Multi variable objective function
- Tayler Series Expansion
- Concept of Optimization
- Maxima
  - Where the slope of the curve is Maximum or the derivative of the function is zero.
- Minima
  - Where the slope of the curve is minimum or the derivative of the function is zero.
- Saddle point
- Generalized optimality conditions
- Necessary conditions
- Sufficient conditions
- Illustrative Example: Optimizing Cone Volume with a Fixed Dimension Sum
- How to find the optimum value?
- Double Derivative or slope of the curve
- Minimization Problem Example(pg 34 in pdf)
- Why derivative should be Zero for optimization?
- Objective Functions
- Equality constraints
- Inequity constraints
- Bounds
 
 ----
 
## 2. Use of Optimization in machine learning
- Curve fitting optimization(Regression)
- Illustrative Example
- Python Problem 1: Find the minimum or maximum of a equation I(x1) or I(x2)
- Python Problem 2: Curve fitting problem
- Python Problem 3: 
----
----

## 2. Gradient and Search-Based Optimisation for Machine Learning

### 2.1 Multi-variable Gradient Based Optimimzation Techniques

#### 2.1.1 Gradient descent
- Example 1: Minimize a function I using gradient descent
#### 2.1.2 Newton's method
- Example 2: Minimize a function I using Newton's method
#### 2.1.3 Comparison of Gradient descent vs. Newton's method
    
----

### 2.2 Constrained Optimization Techniques

#### 2.2.1 Variable Elimination
- Example 3: Minimize a function I using variable elimination method

#### 2.2.2 Lagrange Multipliers
- Example 4: Minimize a function I using variable elimination method
#### 2.2.3 Kuhn Tucker Conditions
- The Kuhn-Tucker conditions, more formally known as the Karush-Kuhn-Tucker (KKT) conditions,
are a set of mathematical rules used to find the optimal solution for certain constrained optimization problems
#### 2.2.4 Penalty functions
- Penalty functions are used in constrained optimization to transform a constrained problem into an 
unconstrained one — by adding a "penalty" to the objective function when the constraints are violated.

----
----

## 3. Linear, Quadratic, and Nonlinear Programming
----
### 3.1 Linear Programming
#### 3.1.1 Standard form of Linear Programming
#### 3.1.2 Canonical form of Linear Equations
----
### 3.2 Methods used for Solving Linear Programming
#### 3.2.1 Simplex Algorithm
#### 3.2.2 Two-phase Simplex Algorithm
#### 3.2.3 Specialized Quadratic Algorithm
#### 3.2.4 Duality
#### 3.2.5 Quadratic Programming
----
### 3.2 Integer Programming
----
### 3.3 Non-Linear Programming

## 4. Multi-objective and Multi-Criteria Decision-Making - Evolutionary Tools

### 4.1 Multi-objective Optimization

## Self-learning Project: Multi Objective Optimisation in Stock investments