
# ğŸ“˜ Regression Cheatsheet: From Linear to Regularization

---

## ğŸ“ˆ Linear Regression

- **Goal**: Predict target `y` from input `x` using a straight line.
- **Equation**: `y = w0 + w1 * x`
- **Assumes** a linear relationship between features and output.
- âœ… Best for: Simple trends, like salary increasing with experience.

---

## ğŸ”„ Polynomial Regression

- **Goal**: Fit a curve (not just a straight line).
- **Equation (degree 2)**: `y = w0 + w1*x + w2*x^2`
- **Still linear in parameters**, but nonlinear in input.
- âœ… Best for: U-shaped, curved patterns in data.

---

## ğŸ§  Underfitting vs Overfitting

| Term         | Meaning                              | Symptoms                      |
|--------------|--------------------------------------|-------------------------------|
| Underfitting | Model too simple                     | Low accuracy on train/test    |
| Overfitting  | Model too complex (memorizes noise)  | High train, low test accuracy |

---

## ğŸ›¡ï¸ Regularization

- **Purpose**: Prevent overfitting by penalizing large weights.
- **Modified Loss**: `Loss = MSE + Î» * penalty`

---

## ğŸ”¹ Ridge Regression (L2)

- **Penalty**: Sum of squared weights: `Î» * âˆ‘wÂ²`
- **Effect**: Shrinks weights but **keeps all features**.
- âœ… Best for: Many **correlated** features.

---

## ğŸ”¸ Lasso Regression (L1)

- **Penalty**: Sum of absolute weights: `Î» * âˆ‘|w|`
- **Effect**: Shrinks some weights to **exactly zero**.
- âœ… Best for: **Feature selection**, sparse models.

---

## ğŸŸ¢ Elastic Net Regression

- **Penalty**: `Î± * L1 + (1 - Î±) * L2`
- **Effect**: Combines Ridge + Lasso
- âœ… Best for: Large datasets with many, possibly correlated features.

---

## ğŸ§ª Example Choices

| Scenario | Best Regression |
|----------|-----------------|
| Straight-line trend | Linear Regression |
| Curved data | Polynomial Regression |
| Many correlated features | Ridge |
| Want fewer features | Lasso |
| Combo case | Elastic Net |

---
