# S20: Introduction to Neural Network

Prompt:

I am preparing for the data science and machine learning exam on topic 

"""
Introduction to Neural Network
"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üìò Introduction to Neural Networks ‚Äì MCQ Set 1 (With Answers & Explanations)

### ‚úÖ 1. What is the primary purpose of a neural network?
**Answer:** C) Detect complex patterns and relationships in data  
**Explanation:** Neural networks excel at learning **non-linear** and **high-dimensional** patterns, which is why they work well in vision, language, and prediction tasks.

---

### ‚úÖ 2. The output of a perceptron is:
**Answer:** B) A binary value based on a threshold  
**Explanation:** The perceptron applies a **step function** and returns either 0 or 1 based on whether the weighted sum of inputs exceeds a threshold.

---

### ‚úÖ 3. Which activation function is commonly used in hidden layers and helps with vanishing gradient issues?
**Answer:** C) ReLU  
**Explanation:** ReLU (Rectified Linear Unit) is preferred in deep networks because it avoids **saturation** and helps **gradient flow** effectively.

---

### ‚úÖ 4. What is the main role of the activation function?
**Answer:** C) To introduce non-linearity into the model  
**Explanation:** Without activation functions, all layers would collapse into a **linear transformation**, limiting the model's power.

---

### ‚úÖ 5. In a neural network, weights are:
**Answer:** C) Parameters that are learned during training  
**Explanation:** Weights are updated via **backpropagation and gradient descent** and determine the influence of inputs on outputs.

---

### ‚úÖ 6. What is the name of the algorithm commonly used to update weights in neural networks?
**Answer:** A) Backpropagation  
**Explanation:** Backpropagation calculates **gradients of the loss** with respect to each weight and updates them to **reduce loss**.

---

### ‚úÖ 7. What is the purpose of backpropagation?
**Answer:** C) Minimize the loss by updating weights through gradients  
**Explanation:** It propagates error backward from output to input, allowing for **efficient weight updates**.

- Back propagation example in simple explanation with mathematical example
https://www.youtube.com/watch?v=QZ8ieXZVjuE

---

### ‚úÖ 8. Which of the following is a common loss function for classification tasks?
**Answer:** B) Cross Entropy  
**Explanation:** Cross-entropy loss compares **predicted class probabilities** with true labels, ideal for classification.

---

### ‚úÖ 9. Overfitting in a neural network can be reduced by:
**Answer:** C) Applying dropout  
**Explanation:** Dropout **randomly deactivates neurons** during training, preventing over-reliance and improving generalization.

---

### ‚úÖ 10. What does the bias term in a neuron do?
**Answer:** C) It allows shifting the activation function  
**Explanation:** Bias helps the activation function to **shift left or right**, improving the model‚Äôs flexibility.

---

### ‚úÖ 11. Which of the following is *not* a benefit of using a multilayer perceptron?
**Answer:** C) Always interpretable  
**Explanation:** MLPs are powerful but are often **black boxes**, lacking transparency in their decision-making.

---

### ‚úÖ 12. Gradient descent is used to:
**Answer:** C) Find the optimal weights by minimizing the loss  
**Explanation:** Gradient descent helps reach the **minimum of the loss function**, guiding weight updates effectively.

---

# üìò Introduction to Neural Networks ‚Äì MCQ Set 2 (With Answers & Explanations)

---

### ‚úÖ 1. What happens during the forward pass in a neural network?
**A)** Gradients are calculated  
**B)** Inputs are transformed layer by layer to generate output  
**C)** Weights are updated  
**D)** Loss is minimized  

**Correct Answer:** B  
üí° **Explanation:** In the forward pass, data moves through the layers and generates predictions. Gradients are only calculated during the backward pass.

---

### ‚ùå 2. In backpropagation, the gradients are calculated using:
**A)** Random initialization  
**B)** Chain rule of calculus  
**C)** Taylor series  
**D)** Second-order derivatives  

**Correct Answer:** B  
üí° **Explanation:** Backpropagation relies on the **chain rule** to compute gradients layer by layer.

---

### ‚úÖ 3. Which of the following best describes the learning rate?
**A)** Number of neurons in each layer  
**B)** Size of the dataset  
**C)** Step size used to update weights  
**D)** Total number of training epochs  

**Correct Answer:** C  
üí° **Explanation:** The learning rate determines **how large a step** the optimizer takes during each weight update.

---

### ‚ùå 4. What problem can occur if the learning rate is too high?
**A)** Model learns very slowly  
**B)** Loss never decreases  
**C)** Model may overshoot minima and never converge  
**D)** Training becomes regularized  

**Correct Answer:** C  
üí° **Explanation:** A high learning rate can cause the optimizer to **bounce around the minimum** or diverge entirely.

---

### ‚úÖ 5. For a binary classification task, the most appropriate output activation and loss function pair is:
**A)** ReLU + MSE  
**B)** Sigmoid + Binary Cross-Entropy  
**C)** Softmax + Categorical Cross-Entropy  
**D)** Tanh + Hinge Loss  

**Correct Answer:** B  
üí° **Explanation:** **Sigmoid** compresses outputs to [0, 1], and **Binary Cross-Entropy** compares them with actual binary labels.

---

### ‚úÖ 6. Xavier initialization is used to:
**A)** Introduce regularization  
**B)** Set bias terms  
**C)** Initialize weights to maintain variance across layers  
**D)** Normalize inputs  

**Correct Answer:** C  
üí° **Explanation:** Xavier (Glorot) initialization helps **keep activations stable** across layers, aiding convergence.

---

### ‚úÖ 7. Which of the following is true about a deep neural network with too many layers?
**A)** Always more accurate  
**B)** Trains faster  
**C)** May suffer from vanishing gradients  
**D)** Requires fewer parameters  

**Correct Answer:** C  
üí° **Explanation:** Deeper networks can experience **vanishing gradients**, especially with sigmoid or tanh activations.

---

### ‚úÖ 8. Which technique randomly disables neurons during training?
**A)** Normalization  
**B)** Max Pooling  
**C)** Dropout  
**D)** Activation Clipping  

**Correct Answer:** C  
üí° **Explanation:** Dropout helps prevent overfitting by **randomly turning off neurons** during each training pass.

---

### ‚ùå 9. Which of the following does NOT affect the capacity of a neural network?
**A)** Number of layers  
**B)** Number of neurons per layer  
**C)** Activation function used  
**D)** Size of the input data  

**Correct Answer:** D  
üí° **Explanation:** Capacity depends on **architecture**, not the input data's dimensionality.

---

### ‚úÖ 10. When is softmax activation usually applied?
**A)** Hidden layers  
**B)** Input layer  
**C)** Binary classification output  
**D)** Multi-class classification output  

**Correct Answer:** D  
üí° **Explanation:** **Softmax** is applied in the **final layer** of multi-class classification models to produce probabilities across all classes.

---

### ‚úÖ 11. L2 regularization (Ridge) helps by:
**A)** Forcing weights to be binary  
**B)** Encouraging sparsity  
**C)** Penalizing large weights to reduce overfitting  
**D)** Reducing the number of neurons  

**Correct Answer:** C  
üí° **Explanation:** L2 adds a **penalty term** to the loss function to discourage large weights and improve generalization.

---

### ‚ùå 12. What is the output of a softmax function?
**A)** A binary value  
**B)** Probabilities that sum to 1  
**C)** One-hot encoded labels  
**D)** Loss gradients  

**Correct Answer:** B  
üí° **Explanation:** Softmax outputs a **vector of probabilities** that sum to 1, ideal for multi-class predictions.

---

---
## Perceptron

### Prompt
I am preparing for the data science and machine learning exam on topic 
"""
Perceptron
"""
I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me a short summary of the topic, followed by MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.
---

##  Perceptron ‚Äì MCQ Set 1 with Answers & Explanations

---

### ‚úÖ 1. What type of problem can a single-layer perceptron solve?
**A)** Non-linear classification  
**B)** Regression  
**C)** Linearly separable classification  
**D)** Multi-class classification  

**Correct Answer:** C  
üí° **Explanation:** A perceptron can solve problems where a straight line or hyperplane can separate the classes ‚Äî i.e., linearly separable data.

---

### ‚úÖ 2. The perceptron algorithm updates weights when:
**A)** The model predicts correctly  
**B)** The weights are negative  
**C)** The prediction is incorrect  
**D)** The bias is zero  

**Correct Answer:** C  
üí° **Explanation:** Weight updates only happen when the model makes a **wrong prediction**.

---

### ‚úÖ 3. Which function is typically used as the activation function in a basic perceptron?
**A)** Sigmoid  
**B)** ReLU  
**C)** Tanh  
**D)** Step function  

**Correct Answer:** D  
üí° **Explanation:** A **step function** is used in the perceptron to produce binary outputs like 0/1 or -1/+1.

---

### ‚úÖ 4. What happens if the data is not linearly separable in a perceptron?
**A)** The algorithm converges slowly  
**B)** The perceptron cannot converge  
**C)** It overfits  
**D)** It ignores the bias term  

**Correct Answer:** B  
üí° **Explanation:** The perceptron algorithm does **not converge** if the data isn't linearly separable.

---

### ‚úÖ 5. The perceptron algorithm guarantees convergence only if:
**A)** The data is normalized  
**B)** The learning rate is small  
**C)** The data is linearly separable  
**D)** The bias is non-zero  

**Correct Answer:** C  
üí° **Explanation:** According to the **Perceptron Convergence Theorem**, convergence is guaranteed only for **linearly separable** data.

---

### ‚ùå 6. What is the role of the bias in a perceptron model?
**A)** To scale the output  
**B)** To allow shifting the decision boundary  
**C)** To normalize inputs  
**D)** To speed up convergence  

**Correct Answer:** B  
üí° **Explanation:** The bias term **shifts the decision boundary**, just like the intercept in linear equations.

---

### ‚úÖ 7. The perceptron learning rule updates weights using which of the following?
**A)** Gradient of the loss function  
**B)** (True label - Predicted label)  
**C)** Backpropagation  
**D)** Mean squared error  

**Correct Answer:** B  
üí° **Explanation:** The update rule is:  
```w = w + Œ∑ * (y_true - y_pred) * x```

---

### ‚úÖ 8. Which of the following problems cannot be solved using a single-layer perceptron?
**A)** AND  
**B)** OR  
**C)** XOR  
**D)** NAND  

**Correct Answer:** C  
üí° **Explanation:** The XOR function is **not linearly separable**. A single-layer perceptron can't solve it.

---

### ‚ùå 9. What is the output range of the basic perceptron?
**A)** Continuous values  
**B)** [0, 1]  
**C)** {-1, +1} or {0, 1} depending on the step function  
**D)** [-1, 1]  

**Correct Answer:** C  
üí° **Explanation:** Output is **discrete**, not continuous. Depends on the step function used.

---

### ‚ùå 10. The perceptron algorithm terminates when:
**A)** Loss becomes zero  
**B)** All weights become positive  
**C)** It converges or reaches maximum iterations  
**D)** All input features are zero  

**Correct Answer:** C  
üí° **Explanation:** The algorithm stops when it **classifies all points correctly** or reaches the **maximum number of iterations**.

---

## ü§ñ Perceptron ‚Äì MCQ Set 2 (Advanced) with Answers & Explanations

---

### ‚úÖ 1. Which of the following logical functions can be represented by a single-layer perceptron?
**A)** XOR  
**B)** NAND  
**C)** XNOR  
**D)** NOT AND (NAND) and OR simultaneously  

**Correct Answer:** B  
üí° **Explanation:** Perceptrons can solve problems like **AND, OR, NOT, NAND** ‚Äî which are linearly separable. **XOR/XNOR** are **not linearly separable**.

---

### ‚úÖ 2. What does the learning rate (Œ∑) control in the perceptron algorithm?
**A)** The complexity of the model  
**B)** How far weights are updated on each error  
**C)** Number of features used  
**D)** Bias initialization  

**Correct Answer:** B  
üí° **Explanation:** Learning rate determines the **step size** in each weight update. It doesn‚Äôt change model structure or bias.

---

### ‚úÖ 3. Consider a perceptron with:  
x‚ÇÅ = 1, x‚ÇÇ = 2, w‚ÇÅ = 0.5, w‚ÇÇ = ‚Äì1, b = 1  
**What is the output using a step function?**

**A)** 0  
**B)** 1  
**C)** ‚Äì1  
**D)** Depends on learning rate  

**Correct Answer:** A  
üí° **Explanation:**  
Compute weighted sum:  
`z = (0.5*1) + (‚Äì1*2) + 1 = ‚Äì0.5`  
Step function ‚Üí Output = 0 (since z < 0)

---

### ‚úÖ 4. Which of the following best describes the decision boundary of a perceptron with two inputs?
**A)** A curve  
**B)** A vertical line  
**C)** A linear line (hyperplane in higher dims)  
**D)** A parabola  

**Correct Answer:** C  
üí° **Explanation:** Perceptrons create **linear decision boundaries** in input space.

---

### ‚úÖ 5. What is the time complexity of the perceptron training algorithm (in worst case)?
**A)** O(n)  
**B)** O(mn)  
**C)** O(1)  
**D)** O(m¬≤)  

**Correct Answer:** B  
üí° **Explanation:**  
m = samples, n = features ‚Üí Each sample can require looping over n features ‚Üí O(mn)

---

### ‚ùå 6. Which factor does *not* influence the convergence of a perceptron?
**A)** Linear separability of data  
**B)** Choice of activation function  
**C)** Learning rate  
**D)** Bias initialization  

**Correct Answer:** B  
üí° **Explanation:** Basic perceptron uses a **fixed step function**. Activation function doesn't change ‚Äî so it doesn't affect convergence.

---

### ‚úÖ 7. What kind of data transformation can help a perceptron solve non-linearly separable problems like XOR?
**A)** Feature duplication  
**B)** Feature scaling  
**C)** Feature mapping to higher dimensions  
**D)** Decreasing learning rate  

**Correct Answer:** C  
üí° **Explanation:** By projecting data into a **higher-dimensional space**, we may make it linearly separable (like in kernel methods or deep networks).

---

### ‚úÖ 8. A perceptron fails to converge even after many iterations. What is the most likely cause?
**A)** Too many features  
**B)** Low bias  
**C)** Non-linearly separable data  
**D)** High learning rate  

**Correct Answer:** C  
üí° **Explanation:** The perceptron convergence theorem only applies to **linearly separable** data.

---

### ‚úÖ 9. Which of the following best describes how a perceptron learns?
**A)** Stores all examples and averages them  
**B)** Uses loss minimization to update weights  
**C)** Updates weights only on incorrect predictions  
**D)** Uses backpropagation to compute gradients  

**Correct Answer:** C  
üí° **Explanation:** The **perceptron rule** updates weights **only when a mistake is made**. No gradient, no loss function involved.

---

### ‚úÖ 10. In practical terms, why is the perceptron rarely used for modern classification tasks?
**A)** It is too slow  
**B)** It lacks activation functions  
**C)** It only works for linearly separable data  
**D)** It requires labeled data  

**Correct Answer:** C  
üí° **Explanation:** Most real-world datasets are **not linearly separable**, making the basic perceptron ineffective.

---


---
## Multi layer Perceptron

## ü§ñ Multi-Layer Perceptron (MLP) ‚Äì MCQ Set 1 with Answers & Explanations

---

### ‚úÖ 1. Which of the following problems can a multi-layer perceptron solve that a single-layer perceptron cannot?

**A)** Linearly separable problems  
**B)** Regression problems  
**C)** XOR problem  
**D)** Binary classification  

**Correct Answer:** C  
üí° **Explanation:** The **XOR problem** is not linearly separable, and a single-layer perceptron fails. MLPs with hidden layers can solve it using non-linear transformations.

---

### ‚úÖ 2. What role does the activation function play in hidden layers of an MLP?

**A)** It increases model size  
**B)** It introduces linearity  
**C)** It reduces training time  
**D)** It introduces non-linearity  

**Correct Answer:** D  
üí° **Explanation:** Activation functions like ReLU, sigmoid, etc., make MLPs capable of learning **non-linear** mappings. Without them, the model is just linear.

---

### ‚úÖ 3. Which activation function is most commonly used in the hidden layers of an MLP?

**A)** Softmax  
**B)** ReLU  
**C)** Sigmoid  
**D)** Step Function  

**Correct Answer:** B  
üí° **Explanation:** **ReLU (Rectified Linear Unit)** is computationally efficient and avoids the vanishing gradient problem, making it ideal for hidden layers.

---

### ‚úÖ 4. Why is ReLU preferred over sigmoid in hidden layers of MLPs?

**A)** It requires fewer neurons  
**B)** It prevents overfitting  
**C)** It avoids vanishing gradients  
**D)** It computes probabilities  

**Correct Answer:** C  
üí° **Explanation:** ReLU maintains stronger gradients for large positive inputs, avoiding the vanishing gradient issue found in sigmoid/tanh.

---

### ‚ùå 5. Which loss function is most appropriate for multi-class classification using MLP?

**A)** Mean Absolute Error  
**B)** Mean Squared Error  
**C)** Binary Cross-Entropy  
**D)** Categorical Cross-Entropy  

**Correct Answer:** D  
üí° **Explanation:** For multi-class classification with Softmax outputs, use **Categorical Cross-Entropy**.  
‚ùå Binary Cross-Entropy is for **2-class problems** only.

---

### ‚úÖ 6. What is the purpose of backpropagation in MLP training?

**A)** Generate new data  
**B)** Prevent overfitting  
**C)** Compute gradients for weight updates  
**D)** Normalize inputs  

**Correct Answer:** C  
üí° **Explanation:** Backpropagation applies the **chain rule** to calculate gradients for all weights so they can be updated via gradient descent.

---

### ‚úÖ 7. In an MLP with 2 hidden layers, how many sets of weights will exist?

**A)** 1  
**B)** 2  
**C)** 3  
**D)** Depends on neurons, not layers  

**Correct Answer:** C  
üí° **Explanation:**  
- Input ‚Üí Hidden Layer 1  
- Hidden Layer 1 ‚Üí Hidden Layer 2  
- Hidden Layer 2 ‚Üí Output  
So, **3 sets of weights** exist.

---

### ‚úÖ 8. What type of learning does an MLP use?

**A)** Unsupervised  
**B)** Reinforcement  
**C)** Supervised  
**D)** Self-supervised  

**Correct Answer:** C  
üí° **Explanation:** MLPs require **labeled data** during training, hence it is a **supervised learning** method.

---

### ‚úÖ 9. The universal approximation theorem states that:

**A)** MLP can replicate CNNs  
**B)** A single hidden layer MLP can approximate any continuous function  
**C)** Only deep networks can solve complex tasks  
**D)** Perceptrons outperform MLPs  

**Correct Answer:** B  
üí° **Explanation:** Even a **single hidden layer MLP** (with enough neurons) can approximate any continuous function on compact input spaces.

---

### ‚ùå 10. Which of the following is *not* a hyperparameter in MLPs?

**A)** Number of hidden layers  
**B)** Learning rate  
**C)** Weights  
**D)** Activation function  

**Correct Answer:** C  
üí° **Explanation:**  
- **Weights** are learned during training ‚Üí not hyperparameters.  
- Hyperparameters are things you **set before training** (layers, learning rate, activation).

---

## Multi layer Perceptron - Structure 

I am preparing for the data science and machine learning exam on topic 

"""
Introduction to Neural Network
"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

### Input Layer, Output Layer and Hidden Layer

I am preparing for the data science and machine learning exam on topic 

"""

Input Layer, Output Layer and Hidden Layer

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

### Epoch, Batch and Mini Batch

I am preparing for the data science and machine learning exam on topic 

"""

Epoch, Batch and Mini Batch

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.



### Nodes, Neurons and Weights in each Layer

I am preparing for the data science and machine learning exam on topic 

"""

Nodes, Neurons and Weights in each Layer

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üß† Revision: Nodes, Neurons & Weights in Neural Network Layers

---

## ‚úÖ Topic Summary

- **Neuron (or Node):** Basic unit of a neural network layer that performs:
  \[
  z = \sum (w_i \cdot x_i) + b
  \]
  followed by an **activation function**.
- **Weights (w):** Determine how much influence a given input has.
- **Bias (b):** Helps shift the activation function output.
- **Layers:**
  - **Input Layer:** Receives features
  - **Hidden Layers:** Perform transformations
  - **Output Layer:** Provides predictions

---

## üìò MCQs with Answers & Explanations

---

### **1. In a fully connected layer with 5 input features and 4 neurons, how many weights are there (excluding bias)?**

**Options:**
- A) 9  
- B) 20  
- C) 5  
- D) 4  

‚úÖ **Answer:** B  
üìò **Explanation:** 5 inputs √ó 4 neurons = **20 weights**.

---

### **2. What does a neuron in a neural network compute?**

**Options:**
- A) A sum of inputs only  
- B) A linear function only  
- C) A weighted sum + activation  
- D) Only bias addition  

‚úÖ **Answer:** C  
üìò **Explanation:** A neuron computes a **weighted sum of inputs plus bias**, passed through an activation function.

---

### **3. What is the total number of parameters (weights + biases) in a layer with 10 inputs and 3 neurons?**

**Options:**
- A) 30  
- B) 33  
- C) 13  
- D) 10  

‚úÖ **Answer:** B  
üìò **Explanation:** Each neuron has 10 weights + 1 bias ‚Üí (10 + 1) √ó 3 = **33 parameters**.

---

### **4. What is the role of a weight in a neural network?**

**Options:**
- A) It activates the neuron  
- B) It stores the input values  
- C) It determines the importance of each input  
- D) It reduces model complexity  

‚úÖ **Answer:** C  
üìò **Explanation:** Weights represent how **important** an input is to a neuron.

---

### **5. If a layer has 8 neurons and each receives input from 6 neurons in the previous layer, how many total weights are there?**

**Options:**
- A) 14  
- B) 48  
- C) 64  
- D) 86  

‚ùå **Correct Answer:** **C**  
üìò **Explanation:** 6 inputs √ó 8 neurons = **48 weights**. Biases not included here.

---

### **6. Which part of a neuron helps shift the activation function left or right?**

**Options:**
- A) Input  
- B) Output  
- C) Weight  
- D) Bias  

‚úÖ **Answer:** D  
üìò **Explanation:** The **bias** term helps shift the activation function.

---

### **7. In a neural network, how are weights learned?**

**Options:**
- A) They are set manually  
- B) Using cross-validation  
- C) By minimizing loss using optimization (e.g., gradient descent)  
- D) They are fixed after initialization  

‚úÖ **Answer:** C  
üìò **Explanation:** Weights are **updated during training** by minimizing the loss function using optimizers.

---

### **8. If the number of neurons in a layer increases, what happens to the number of weights?**

**Options:**
- A) Decreases  
- B) Remains the same  
- C) Increases  
- D) Only bias increases  

‚úÖ **Answer:** C  
üìò **Explanation:** More neurons mean more **connections and weights**.

---

### **9. What does it mean if a weight is close to zero after training?**

**Options:**
- A) It adds more non-linearity  
- B) The input it connects is not important  
- C) Model accuracy improves  
- D) It overfits  

‚ùå **Correct Answer:** **B**  
üìò **Explanation:** A weight close to zero means the **connected input feature isn‚Äôt contributing much**.

---

### **10. A neural network layer with 7 inputs and 2 neurons will have how many biases?**

**Options:**
- A) 7  
- B) 2  
- C) 1  
- D) 9  

‚úÖ **Answer:** B  
üìò **Explanation:** One bias per neuron ‚Üí **2 biases** total.

---

## ‚úÖ Final Score: **8 / 10**

Focus areas for improvement:
- **Weight interpretation after training (Q9)**
- **Weight calculation in multi-neuron layers (Q5)**

---



---
## Classical Activation function and when to use them

I am preparing for the data science and machine learning exam on topic 

"""

Classical activation function and when to use them

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üî¢ Classical Activation Functions ‚Äì MCQ Revision (IIT Delhi Prep)

---

## üìò Topic Summary

**Classical Activation Functions Covered:**
- **Linear (Identity):** Used in regression, no non-linearity  
- **Sigmoid:** Output range (0, 1), used in binary classification output  
- **Tanh:** Output range (‚Äì1, 1), zero-centered  
- **ReLU:** Popular in hidden layers, outputs max(0, x)  
- **Leaky ReLU:** Variant of ReLU that avoids dying neurons by allowing small gradient for negative inputs

---

## ‚úÖ MCQs with Correct Answers & Explanations

---

### **1. Which of the following activation functions is linear?**

**Options:**
- A) Sigmoid  
- B) Tanh  
- C) ReLU  
- D) Identity  

‚úÖ **Answer:** D  
üìò **Explanation:** The identity function \( f(x) = x \) is linear, used in regression tasks.

---

### **2. Which activation function is commonly used in the output layer of a binary classification task?**

**Options:**
- A) ReLU  
- B) Tanh  
- C) Sigmoid  
- D) Linear  

‚úÖ **Answer:** C  
üìò **Explanation:** Sigmoid outputs values between 0 and 1 ‚Äî ideal for binary classification.

---

### **3. What is a key reason to avoid sigmoid in deep hidden layers?**

**Options:**
- A) High computation cost  
- B) Exploding gradient  
- C) Vanishing gradient  
- D) Too many parameters  

‚úÖ **Answer:** C  
üìò **Explanation:** Sigmoid saturates for large inputs, causing gradients to vanish and slow learning.

---

### **4. What is the output range of the tanh function?**

**Options:**
- A) [0, 1]  
- B) (‚Äì1, 1)  
- C) (0, ‚àû)  
- D) [‚Äì‚àû, ‚àû]  

‚úÖ **Answer:** B  
üìò **Explanation:** tanh outputs are in the range (‚Äì1, 1), centered around 0.

---

### **5. Why is ReLU preferred over sigmoid/tanh in hidden layers of deep networks?**

**Options:**
- A) It‚Äôs smoother  
- B) It avoids saturation for positive inputs  
- C) It outputs probabilities  
- D) It centers data around zero  

‚úÖ **Answer:** B  
üìò **Explanation:** ReLU avoids saturation for positive values, helping gradient flow better during training.

---

### **6. What is the output of a ReLU activation for input x = ‚Äì5?**

**Options:**
- A) ‚Äì5  
- B) 0  
- C) 1  
- D) Undefined  

‚úÖ **Answer:** B  
üìò **Explanation:** ReLU returns 0 for all negative inputs.

---

### **7. Leaky ReLU differs from ReLU because it:**

**Options:**
- A) Adds noise  
- B) Is used in output layers  
- C) Passes small negative gradients  
- D) Requires more memory  

‚úÖ **Answer:** C  
üìò **Explanation:** Leaky ReLU allows a small slope for negative inputs to avoid neuron death.

---

### **8. Which function is most likely to produce sparse activations (i.e., many zeros)?**

**Options:**
- A) Sigmoid  
- B) Tanh  
- C) ReLU  
- D) Linear  

‚úÖ **Answer:** C  
üìò **Explanation:** ReLU outputs 0 for all negative inputs, making many activations zero ‚Äî hence "sparse."

---

### **9. Which activation function is best for output layer in regression?**

**Options:**
- A) ReLU  
- B) Sigmoid  
- C) Tanh  
- D) Linear  

‚úÖ **Answer:** D  
üìò **Explanation:** In regression, outputs can be unbounded, so the linear function is ideal.

---

### **10. What is a common issue with ReLU in practice?**

**Options:**
- A) Always outputs zero  
- B) Outputs negative values  
- C) Can cause neurons to stop learning  
- D) Is not differentiable at zero  

‚úÖ **Answer:** C  
üìò **Explanation:** If weights drive ReLU into the negative region, the neuron can "die" and stop updating.

---

## üß† Quick Comparison Table

| Function    | Range      | Zero-Centered | Use Case                  | Drawback                   |
|-------------|------------|----------------|----------------------------|-----------------------------|
| Linear      | (‚Äì‚àû, ‚àû)    | ‚úÖ Yes          | Regression output          | No non-linearity            |
| Sigmoid     | (0, 1)     | ‚ùå No           | Binary classification      | Vanishing gradients         |
| tanh        | (‚Äì1, 1)    | ‚úÖ Yes          | Hidden layers              | Vanishing gradients         |
| ReLU        | [0, ‚àû)     | ‚ùå No           | Deep networks (hidden)     | Dying neurons (negatives)   |
| Leaky ReLU  | (‚Äì‚àû, ‚àû)    | ‚ùå No           | Deep networks (hidden)     | Slight complexity increase  |

---

---
### Sigmoid Activation Function

I am preparing for the data science and machine learning exam on topic 

"""

sigmoid activation function

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üß† Sigmoid Activation Function ‚Äì MCQ Revision Sheet

---

## ‚úÖ Quick Summary

- **Formula:**  
  \[
  \sigma(x) = \frac{1}{1 + e^{-x}}
  \]

- **Range:** (0, 1)  
- **Shape:** S-shaped (sigmoidal curve)  
- **Key Use:** Output layer in binary classification  
- **Limitation:** Vanishing gradient for large/small values of x

---

## üìò MCQs with Answers and Explanations

---

### **1. What is the output range of the sigmoid function?**

**Options:**
- A) [‚Äì1, 1]  
- B) [0, 1]  
- C) [0, ‚àû)  
- D) (‚Äì‚àû, ‚àû)  

‚úÖ **Answer:** B  
üìò **Explanation:** The sigmoid function squashes inputs to a range between 0 and 1.

---

### **2. What is the formula for the sigmoid activation function?**

**Options:**
- A) \( \frac{e^x - e^{-x}}{e^x + e^{-x}} \)  
- B) \( \max(0, x) \)  
- C) \( \frac{1}{1 + e^{-x}} \)  
- D) \( x \)  

‚úÖ **Answer:** C  
üìò **Explanation:** This formula defines the sigmoid function.

---

### **3. For large positive values of x, the sigmoid output will be close to:**

**Options:**
- A) 0  
- B) 0.5  
- C) 1  
- D) ‚àû  

‚úÖ **Answer:** C  
üìò **Explanation:** As x becomes very large, the output of sigmoid approaches 1.

---

### **4. In which layer is the sigmoid activation function most appropriately used?**

**Options:**
- A) Input layer  
- B) Hidden layer in CNN  
- C) Output layer of binary classification  
- D) Output layer of regression  

‚úÖ **Answer:** C  
üìò **Explanation:** Sigmoid is used in binary classification output layers since it returns a probability-like output between 0 and 1.

---

### **5. What is a key disadvantage of using sigmoid in deep networks?**

**Options:**
- A) Exploding gradient  
- B) Vanishing gradient  
- C) Gradient clipping  
- D) It doesn‚Äôt normalize output  

‚úÖ **Answer:** B  
üìò **Explanation:** The gradient becomes very small for large positive or negative inputs, leading to slow learning.

---

### **6. What is the derivative of the sigmoid function?**

**Options:**
- A) \( \sigma(x)^2 \)  
- B) \( \sigma(x)(1 - \sigma(x)) \)  
- C) \( 1 - \sigma(x)^2 \)  
- D) \( e^{-x} \)  

‚úÖ **Answer:** B  
üìò **Explanation:**  
\[
\frac{d}{dx} \sigma(x) = \sigma(x)(1 - \sigma(x))
\]

---

### **7. Sigmoid is NOT ideal in hidden layers because:**

**Options:**
- A) It‚Äôs slow to compute  
- B) It has no activation  
- C) Its gradients become very small for extreme inputs  
- D) It‚Äôs not differentiable  

‚úÖ **Answer:** C  
üìò **Explanation:** For very large or small inputs, sigmoid outputs saturate and gradients vanish, slowing down training.

---

### **8. What is the value of sigmoid(0)?**

**Options:**
- A) 0  
- B) 0.25  
- C) 0.5  
- D) 1  

‚úÖ **Answer:** C  
üìò **Explanation:**  
\[
\sigma(0) = \frac{1}{1 + e^{0}} = 0.5
\]

---

### **9. Which of the following is a property of the sigmoid function?**

**Options:**
- A) Zero-centered output  
- B) Output always positive  
- C) Output only in negative values  
- D) Output always 1  

‚úÖ **Answer:** B  
üìò **Explanation:** Sigmoid outputs are always between 0 and 1 ‚Äî so always positive.

---

### **10. Why is sigmoid suitable for binary classification outputs?**

**Options:**
- A) Because it outputs negative values  
- B) Because it gives a probability between 0 and 1  
- C) Because it forces sparsity  
- D) Because it‚Äôs linear  

‚úÖ **Answer:** B  
üìò **Explanation:** Sigmoid output can be interpreted as the probability of a class ‚Äî perfect for binary classification.

---

## üß† Summary Table ‚Äì Activation Function Comparison

| Property           | Sigmoid          | tanh             | ReLU             |
|--------------------|------------------|------------------|------------------|
| Output Range       | (0, 1)           | (‚Äì1, 1)          | [0, ‚àû)           |
| Zero-centered?     | ‚ùå No             | ‚úÖ Yes            | ‚ùå No             |
| Vanishing Gradient | ‚úÖ Yes            | ‚úÖ Yes            | ‚ùå Rarely         |
| Use Case           | Binary classification output | Hidden layers | Hidden layers |

---

---
### tanh Activation Function

I am preparing for the data science and machine learning exam on topic 

"""

tanh activation function

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üß† tanh Activation Function ‚Äì MCQ Revision Sheet

---

## ‚úÖ Quick Summary

- **Function:**  
  \[
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  \]
- **Output Range:** (‚Äì1, 1)
- **Zero-centered:** Yes ‚úÖ  
- **Use Case:** Often used in **hidden layers** when symmetry around zero helps convergence.
- **Main Drawback:** Can **saturate** for large inputs ‚Üí leads to **vanishing gradients**.

---

## üìò MCQs with Answers and Explanations

---

### **1. What is the mathematical range of the tanh function?**

**Options:**
- A) [0, 1]  
- B) (‚Äì‚àû, ‚àû)  
- C) (‚Äì1, 1)  
- D) [0, ‚àû)

‚úÖ **Answer:** C  
üìò **Explanation:** The `tanh` function outputs values between ‚Äì1 and 1, unlike sigmoid which is [0, 1].

---

### **2. What is the value of tanh(0)?**

**Options:**
- A) ‚Äì1  
- B) 0  
- C) 1  
- D) Undefined  

‚úÖ **Answer:** B  
üìò **Explanation:** `tanh(0) = 0`, since the function is symmetric and zero-centered.

---

### **3. What kind of function is tanh?**

**Options:**
- A) Linear  
- B) Non-linear  
- C) Step  
- D) Identity  

‚úÖ **Answer:** B  
üìò **Explanation:** It‚Äôs a non-linear function, which allows neural networks to model complex patterns.

---

### **4. Compared to sigmoid, what advantage does tanh offer?**

**Options:**
- A) Higher output range  
- B) Lower computation time  
- C) Zero-centered outputs  
- D) Better at classification  

‚úÖ **Answer:** C  
üìò **Explanation:** `tanh` outputs are centered around 0, which helps **faster convergence** during training compared to sigmoid.

---

### **5. What is a major disadvantage of tanh activation?**

**Options:**
- A) Exploding gradients  
- B) Always positive outputs  
- C) Computational instability  
- D) Vanishing gradient for large inputs  

‚úÖ **Answer:** D  
üìò **Explanation:** Like sigmoid, `tanh` **saturates** for very large positive or negative inputs, causing the gradient to vanish.

---

### **6. tanh is typically used in:**

**Options:**
- A) Regression output layers  
- B) Classification output layers  
- C) Hidden layers  
- D) Input layers  

‚ùå **Your Answer:** A  
‚úÖ **Correct Answer:** C  
üìò **Explanation:** `tanh` is typically used in **hidden layers**, not output layers (where linear or softmax is used).

---

### **7. The derivative of tanh(x) is:**

**Options:**
- A) \( 1 - \tanh^2(x) \)  
- B) \( \tanh(x) \)  
- C) \( 1 + \tanh(x) \)  
- D) \( x \cdot \tanh(x) \)  

‚ùå **Your Answer:** Not sure  
‚úÖ **Correct Answer:** A  
üìò **Explanation:**  
\[
\frac{d}{dx} \tanh(x) = 1 - \tanh^2(x)
\]

---

### **8. What does it mean for a function to be "zero-centered"?**

**Options:**
- A) Outputs range from 0 to 1  
- B) Output mean is close to 0  
- C) Output range is always negative  
- D) Always returns 0  

‚úÖ **Answer:** B  
üìò **Explanation:** A zero-centered function like `tanh` has outputs symmetrically distributed around zero.

---

### **9. Which of the following statements is TRUE?**

**Options:**
- A) tanh causes sparse activation  
- B) tanh is better than ReLU in deep networks  
- C) tanh performs best for large values of input  
- D) tanh can saturate and slow training  

‚ùå **Your Answer:** A  
‚úÖ **Correct Answer:** D  
üìò **Explanation:** tanh **can saturate** for large inputs ‚Üí slows learning due to very small gradients.

---

### **10. Which function is more likely to produce negative outputs: sigmoid or tanh?**

**Options:**
- A) Sigmoid  
- B) tanh  
- C) Both equally  
- D) None  

‚úÖ **Answer:** B  
üìò **Explanation:** Sigmoid ranges [0, 1] ‚Üí **never negative**. `tanh` ranges (‚Äì1, 1) ‚Üí **can output negatives**.

---

## üß† Key Takeaways

| Feature           | tanh        | Sigmoid     | ReLU        |
|------------------|-------------|-------------|-------------|
| Output Range     | (‚Äì1, 1)     | (0, 1)      | [0, ‚àû)      |
| Zero-centered?   | ‚úÖ Yes       | ‚ùå No        | ‚ùå No        |
| Derivative Max   | 1           | 0.25        | 1 (x > 0)   |
| Saturation Risk  | High        | High        | Low         |
| Use In           | Hidden Layer| Output (binary)| Hidden Layer |

---


---
### ReLU Activation Function

I am preparing for the data science and machine learning exam on topic 

"""

ReLU activation function

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üöÄ ReLU Activation Function ‚Äì MCQ Revision Sheet

---

## üß† Quick Summary

- **ReLU (Rectified Linear Unit)**:  
  \[
  f(x) = \max(0, x)
  \]
- Introduces **non-linearity** without saturation for positive inputs.
- Helps avoid **vanishing gradient problem** (common with sigmoid/tanh).
- **Used in hidden layers** of most deep neural networks.
- **Variants** include: Leaky ReLU, Parametric ReLU, ReLU6.

---

## ‚úÖ MCQs with Answers & Explanations

---

### **1. What is the formula of the ReLU activation function?**

**Options:**
- A) \( f(x) = \tanh(x) \)  
- B) \( f(x) = \max(0, x) \)  
- C) \( f(x) = \frac{1}{1 + e^{-x}} \)  
- D) \( f(x) = \min(0, x) \)  

‚úÖ **Answer:** B  
üìò **Explanation:** ReLU returns the input if it‚Äôs positive, otherwise returns 0.

---

### **2. What is the main advantage of ReLU over sigmoid or tanh in deep networks?**

**Options:**
- A) It's smoother  
- B) It's symmetric  
- C) It avoids vanishing gradient for positive values  
- D) It‚Äôs probabilistic  

‚úÖ **Answer:** C  
üìò **Explanation:** ReLU maintains a strong gradient for positive inputs, improving learning speed and depth.

---

### **3. What is the output of ReLU when the input is ‚Äì2.5?**

**Options:**
- A) ‚Äì2.5  
- B) 0  
- C) 1  
- D) Undefined  

‚úÖ **Answer:** B  
üìò **Explanation:** Negative values are mapped to 0 by ReLU.

---

### **4. Which of the following is a drawback of ReLU?**

**Options:**
- A) It squashes gradients  
- B) Exploding gradients  
- C) Dying ReLU problem  
- D) It outputs in [0, 1]  

‚úÖ **Answer:** C  
üìò **Explanation:** Neurons can "die" by always outputting 0 if they enter a no-gradient state due to negative inputs.

---

### **5. Which layer is ReLU typically used in?**

**Options:**
- A) Output layer of regression  
- B) Output layer of classification  
- C) Hidden layers  
- D) Input layer  

‚úÖ **Answer:** C  
üìò **Explanation:** ReLU is the most common activation in **hidden layers** due to its simplicity and effectiveness.

---

### **6. What happens to negative input values in ReLU?**

**Options:**
- A) Passed as-is  
- B) Squared  
- C) Become 0  
- D) Multiplied by a small constant  

‚úÖ **Answer:** C  
üìò **Explanation:** For any \( x < 0 \), ReLU outputs 0.

---

### **7. Which activation function is used to solve the "dying ReLU" problem?**

**Options:**
- A) Leaky ReLU  
- B) Softmax  
- C) Tanh  
- D) Identity  

‚úÖ **Answer:** A  
üìò **Explanation:** Leaky ReLU assigns a small slope (e.g., 0.01x) for negative values to prevent neurons from dying.

---

### **8. What is the derivative of ReLU for positive input values?**

**Options:**
- A) 1  
- B) 0  
- C) Undefined  
- D) Input value itself  

‚úÖ **Answer:** A  
üìò **Explanation:** For \( x > 0 \), the derivative of ReLU is constant at 1.

---

### **9. ReLU is a:**

**Options:**
- A) Linear activation function  
- B) Non-linear activation function  
- C) Saturated activation function  
- D) Sigmoid variant  

‚úÖ **Answer:** B  
üìò **Explanation:** ReLU is **non-linear**, despite being piecewise linear‚Äîit allows deep networks to learn complex mappings.

---

### **10. What is the range of output values for ReLU?**

**Options:**
- A) [0, 1]  
- B) [‚Äì1, 1]  
- C) [0, ‚àû)  
- D) (‚Äì‚àû, ‚àû)  

‚ùå **Your Answer:** A  
‚úÖ **Correct Answer:** C  
üìò **Explanation:** ReLU can return **any non-negative value** from 0 to ‚àû. It does not limit the upper bound like sigmoid or tanh.

---

## üîÅ Key Takeaways

- ReLU is **computationally efficient**, avoids saturation, and works well with deep networks.
- It **outputs zero for negatives** and **retains positives**, making it sparse and fast.
- Avoid ReLU in output layers for regression‚Äî**use linear** instead.
- **Leaky ReLU** is a good fix for dying ReLU neurons.

---


---
### Linear Activation Function

I am preparing for the data science and machine learning exam on topic 

"""

Linear activation function

"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üìò Linear Activation Function ‚Äì MCQ Revision

---

## ‚úÖ Summary

- A **Linear Activation Function** is simply:  
  \[
  f(x) = x
  \]

- It passes input as-is without transformation.
- Commonly used in **output layers of regression models**.
- Rarely used in **hidden layers**, since it introduces no non-linearity.

---

## üß† MCQs with Answers & Explanations

---

### **1. What is the formula for a linear activation function?**

**Options:**
- A) \( \frac{1}{1 + e^{-x}} \)  
- B) \( \max(0, x) \)  
- C) \( x \)  
- D) \( \tanh(x) \)  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** Linear activation is the identity function: \( f(x) = x \).

---

### **2. In which of the following scenarios is a linear activation function typically used?**

**Options:**
- A) Binary classification  
- B) Multi-class classification  
- C) Regression output layer  
- D) Hidden layers in CNNs  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** In regression, continuous values are predicted ‚Äî so no transformation is needed in the final layer.

---

### **3. What is the main limitation of a linear activation function in hidden layers?**

**Options:**
- A) Too steep gradient  
- B) Exploding output  
- C) No non-linearity introduced  
- D) Output is always negative  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** Without non-linearity, the model cannot learn complex functions ‚Äî just a stacked linear combination.

---

### **4. What does the derivative of a linear activation function look like?**

**Options:**
- A) Varies with input  
- B) Equal to 0  
- C) Equal to 1  
- D) Equal to input squared  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** Derivative of \( f(x) = x \) is always **1**, which makes backprop simple but not very expressive.

---

### **5. Which of the following statements is TRUE about linear activation functions?**

**Options:**
- A) They are commonly used in hidden layers  
- B) They allow modeling of complex relationships  
- C) They provide non-linear transformation  
- D) They can be useful in the output layer of regression models  

‚úÖ **Correct Answer:** D  
üìò **Explanation:** They are useful in regression models because the target output is continuous.

---

### **6. Using a linear activation function in all layers of a neural network leads to:**

**Options:**
- A) High model accuracy  
- B) Deep representations  
- C) A model that behaves like a linear model  
- D) Better generalization  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** The entire network becomes equivalent to a **single linear transformation**.

---

### **7. Linear activation functions are sometimes referred to as:**

**Options:**
- A) ReLU  
- B) Step function  
- C) Identity function  
- D) Log-sigmoid  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** A linear activation function is also known as the **identity function**.

---

### **8. What is the output range of a linear activation function?**

**Options:**
- A) [0, 1]  
- B) [‚Äì1, 1]  
- C) Only positive numbers  
- D) (‚Äì‚àû, ‚àû)  

‚úÖ **Correct Answer:** D  
üìò **Explanation:** The output of \( f(x) = x \) is unbounded in both directions.

---

### **9. Why are non-linear activation functions preferred in deep networks over linear ones?**

**Options:**
- A) They reduce vanishing gradients  
- B) They introduce sparsity  
- C) They allow learning of complex patterns  
- D) They are easier to com

---
### When to use Softmax over sigmoid activation function?

I am preparing for the data science and machine learning exam on topic 

"""

Softmax, softmax vs sigmoid
"""

I want to score maximum marks. The exam is MCQ based, the exam is conducted by IIT Delhi.
In the last exam I scored 16/24. This time i want to score 24/24. 
Provide me MCQ based questions for practice and later solutions separately to cover the full scope
of questions on this topic.

# üß† Softmax vs Sigmoid ‚Äì Final MCQ Revision (with Answers & Explanations)

---

### **1. What is the primary purpose of the softmax function in a neural network?**

**Options:**
- A) Normalize outputs between 0 and 1 for binary classification  
- B) Convert logits into class probabilities for multi-class classification  
- C) Reduce overfitting in large networks  
- D) Minimize cross-entropy loss  

‚úÖ **Correct Answer:** B  
üìò **Explanation:** Softmax converts raw outputs (logits) into a probability distribution across multiple classes. It‚Äôs used in multi-class classification.

---

### **2. The softmax function outputs values that:**

**Options:**
- A) Are always greater than 1  
- B) Can be negative  
- C) Are between 0 and 1 and sum up to 1  
- D) Are always equal  

‚úÖ **Correct Answer:** C  
üìò **Explanation:** Softmax outputs probabilities between 0 and 1 that add up to 1 across all classes.

---

### **3. When is the sigmoid function preferred over softmax?**

**Options:**
- A) When you have more than 3 output classes  
- B) When your model is regression based  
- C) In binary classification pr

---
## Loss Functions - Regression
  - Question on Loss functions (MAE, MSE, and RMSE)

# üìò Revision ‚Äì Loss Functions in Regression (MAE, MSE, RMSE)

---

## üß† Summary of Key Loss Functions

| Loss Function | Formula | Pros | Cons | Use Case |
|---------------|---------|------|------|----------|
| **MAE** (Mean Absolute Error) | \( \frac{1}{n} \sum |y_i - \hat{y}_i| \) | Robust to outliers | Not differentiable at 0 | Use when outliers are not critical |
| **MSE** (Mean Squared Error) | \( \frac{1}{n} \sum (y_i - \hat{y}_i)^2 \) | Smooth gradients, penalizes large errors | Very sensitive to outliers | Emphasize large error correction |
| **RMSE** (Root Mean Squared Error) | \( \sqrt{\frac{1}{n} \sum (y_i - \hat{y}_i)^2} \) | Same units as original | Still sensitive to outliers | Interpretability in same scale |

---

## üìó MCQs with Correct Answers and Explanations

---

### **1. Which loss function penalizes large errors more severely?**

**Options:**  
A) MAE  
B) MSE  
C) RMSE  
D) Hinge Loss  

‚úÖ **Answer:** B  
üìò **Explanation:** MSE squares the error ‚Üí increases penalty for large deviations.

---

### **2. If all errors are small, how will MAE and MSE compare?**

**Options:**  
A) MAE will be larger  
B) MSE will be larger  
C) They will be equal  
D) RMSE will be zero  

‚úÖ **Answer:** B  
üìò **Explanation:** Even small errors squared will slightly increase MSE over MAE.

---

### **3. Which loss function is most sensitive to outliers?**

**Options:**  
A) MAE  
B) MSE  
C) Cross Entropy  
D) Huber Loss  

‚úÖ **Answer:** B  
üìò **Explanation:** MSE exaggerates the effect of large errors (due to squaring).

---

### **4. What is the RMSE of predictions [3, 5, 2] and ground truth [4, 4, 2]?**

**Options:**  
A) 0.5  
B) 1  
C) ‚àö2 ‚âà 1.41  
D) 2  

‚ùå **Correct Answer:** None of the above  
üìò **Calculation:**  
Errors = [-1, +1, 0]  
Squared = [1, 1, 0]  
MSE = (1 + 1 + 0) / 3 = 0.6667  
RMSE = ‚àö0.6667 ‚âà **0.8165**

---

### **5. What is the main reason to prefer MAE over MSE?**

**Options:**  
A) MAE is faster to compute  
B) MAE handles categorical data  
C) MAE is less sensitive to outliers  
D) MAE always has smaller values  

‚úÖ **Answer:** C  
üìò **Explanation:** MAE is **less sensitive** to outliers than MSE.

---

### **6. What‚Äôs the difference between RMSE and MSE?**

**Options:**  
A) RMSE divides instead of squares  
B) RMSE uses absolute values  
C) RMSE is the square root of MSE  
D) There‚Äôs no difference  

‚úÖ **Answer:** C  
üìò **Explanation:** RMSE = ‚àö(MSE), helps in interpretability in original units.

---

### **7. MSE is most appropriate when:**

**Options:**  
A) You want to ignore large errors  
B) You want to heavily penalize large errors  
C) You want to apply classification  
D) The target is categorical  

‚úÖ **Answer:** B  
üìò **Explanation:** MSE is best when **emphasizing large error correction**.

---

### **8. If one prediction is 1000 and the true value is 10, which function will penalize it most?**

**Options:**  
A) MAE  
B) RMSE  
C) MSE  
D) L1 Loss  

‚úÖ **Answer:** C  
üìò **Explanation:** **MSE**: error¬≤ = (990)¬≤ = **980100** ‚Äî massive penalty.

---

## ‚úÖ Score: 7 / 8  
üü° **To Review:**  
- Understand RMSE computation (numerical)  
- When to prefer MAE over MSE

---


## Loss function - Classification
- Cross Entropy

# üìò Revision: Cross-Entropy Loss ‚Äì Classification

---

## üß† Summary

### üîπ Binary Cross-Entropy (Log Loss)
\[
\text{Loss} = -\left[y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y})\right]
\]
- Used in binary classification
- Requires **Sigmoid** activation for predicted probability

### üîπ Categorical Cross-Entropy
\[
\text{Loss} = -\sum y_i \cdot \log(\hat{y}_i)
\]
- Used for multi-class classification
- Requires **Softmax** to convert logits into a probability distribution

---

## ‚úÖ MCQs with Correct Answers and Explanations

---

### **1. Cross-entropy loss is mainly used in which type of tasks?**

**Options:**  
A) Regression  
B) Binary classification  
C) Clustering  
D) Dimensionality reduction  

‚úÖ **Answer:** B  
üìò **Explanation:** Binary cross-entropy is widely used for **classification tasks**.

---

### **2. Which activation function is typically used before applying binary cross-entropy loss?**

**Options:**  
A) Tanh  
B) ReLU  
C) Sigmoid  
D) Softmax  

‚úÖ **Answer:** C  
üìò **Explanation:** **Sigmoid** converts model outputs to probabilities for binary classification.

---

### **3. What happens to cross-entropy loss when predicted probability is close to the true label?**

**Options:**  
A) Loss increases  
B) Loss becomes zero  
C) Loss is constant  
D) Loss is undefined  

‚úÖ **Answer:** B  
üìò **Explanation:** If prediction matches label (e.g., 1 vs 0.999), the **loss is minimal** (approaches zero).

---

### **4. What will be the binary cross-entropy loss if the predicted probability is 0.99 and true label is 1?**

**Options:**  
A) High  
B) Low  
C) Infinite  
D) Zero  

‚úÖ **Answer:** B  
üìò **Explanation:** Prediction is close to the true value ‚Üí loss is **low but not zero**.

---

### **5. In multi-class classification, which function is paired with categorical cross-entropy?**

**Options:**  
A) Sigmoid  
B) ReLU  
C) Softmax  
D) Linear  

‚úÖ **Answer:** C  
üìò **Explanation:** **Softmax** normalizes outputs into a probability distribution.

---

### **6. Which of the following best describes cross-entropy loss?**

**Options:**  
A) Distance between vectors  
B) Measure of variance  
C) Measure of dissimilarity between probability distributions  
D) Margin-based loss  

‚úÖ **Answer:** C  
üìò **Explanation:** Cross-entropy measures **how different** two probability distributions are.

---

### **7. When using cross-entropy loss, predictions should be:**

**Options:**  
A) Raw scores  
B) One-hot vectors  
C) Probabilities  
D) Labels only  

‚úÖ **Answer:** C  
üìò **Explanation:** Model should output **probabilities**, not hard labels.

---

### **8. Cross-entropy loss is undefined when:**

**Options:**  
A) Predictions are 1  
B) Predictions are 0  
C) Log(0) occurs  
D) Predictions are correct  

‚úÖ **Answer:** C  
üìò **Explanation:** **Log(0)** is undefined, causing numerical errors.

---

### **9. For binary classification with cross-entropy, what is the ideal predicted value for y = 0?**

**Options:**  
A) 1  
B) 0.5  
C) 0  
D) Any number  

‚úÖ **Answer:** C  
üìò **Explanation:** Ideal prediction for true label 0 is **probability = 0**.

---

### **10. Which loss is most appropriate for multi-class classification with mutually exclusive classes?**

**Options:**  
A) MSE  
B) Hinge Loss  
C) Categorical Cross-Entropy  
D) Binary Cross-Entropy  

‚úÖ **Answer:** C  
üìò **Explanation:** **Categorical cross-entropy** works with **Softmax** for multi-class classification.

---

## ‚úÖ Final Score: 8 / 10  
- üîÅ **To Review:**  
  - Q4: Predicted 0.99 vs True 1 ‚â† zero loss  
  - Q7: Model outputs should be **probabilities**, not one-hot vectors

---


## Hyperparameter Tuning

# üìò Hyperparameter Tuning ‚Äì Revision Notes & MCQs

---

## üîπ What Are Hyperparameters?

- Configuration values set **before training** the model.
- Not learned from data like weights or biases.
- Examples:
  - Learning rate
  - Number of layers/neurons
  - Batch size
  - Epochs
  - Dropout rate
  - Regularization (e.g., alpha in Ridge)

---

## üîπ Popular Hyperparameter Tuning Methods

| Method                | Description |
|----------------------|-------------|
| **Grid Search**       | Exhaustively tries all parameter combinations |
| **Random Search**     | Randomly samples from parameter grid |
| **Bayesian Optimization** | Uses previous results to predict next best configuration |
| **Manual Tuning**     | Trial-and-error based on experience |
| **Tools**: `GridSearchCV`, `RandomizedSearchCV`, Optuna, Ray Tune |

---

## ‚úÖ MCQs with Correct Answers and Explanations

---

### **1. Which of the following is a hyperparameter?**  
**Options:** A) Weight of a neuron B) Loss value C) Learning rate D) Training accuracy  
‚úÖ **Answer:** C  
üìò **Explanation:** Learning rate is a hyperparameter set before training.

---

### **2. Which method searches exhaustively through all combinations?**  
**Options:** A) Random Search B) Bayesian Optimization C) Grid Search D) Evolutionary Algorithm  
‚úÖ **Answer:** C  
üìò **Explanation:** Grid Search evaluates every parameter combination.

---

### **3. What is a major drawback of Grid Search?**  
**Options:** A) It‚Äôs random B) Can‚Äôt tune multiple parameters C) Computationally expensive D) Only works on classification  
‚úÖ **Answer:** C  
üìò **Explanation:** Grid search is slow when tuning many parameters.

---

### **4. What is the main advantage of Random Search over Grid Search?**  
**Options:** A) Better accuracy always B) Works on small datasets C) Finds global minimum D) Faster and still effective  
‚úÖ **Answer:** D  
üìò **Explanation:** Random search is faster and nearly as effective in practice.

---

### **5. Which library provides GridSearchCV?**  
**Options:** A) NumPy B) Matplotlib C) scikit-learn D) TensorFlow  
‚úÖ **Answer:** C  
üìò **Explanation:** GridSearchCV is a tuning function from `sklearn`.

---

### **6. Bayesian optimization differs from grid/random search because it:**  
**Options:** A) Uses dropout B) Chooses all combinations C) Uses previous results to guide next trial D) Ignores loss values  
‚úÖ **Answer:** C  
üìò **Explanation:** Bayesian optimization is smart ‚Äì learns from previous evaluations.

---

### **7. What is the main objective of hyperparameter tuning?**  
**Options:** A) Reduce training data B) Find best model weights C) Maximize test error D) Improve model performance  
‚úÖ **Answer:** D  
üìò **Explanation:** Tuning aims to get the **best validation/test accuracy**.

---

### **8. Which of the following is *not* a hyperparameter?**  
**Options:** A) Number of layers B) Epochs C) Regularization D) Model weights  
‚úÖ **Answer:** D  
üìò **Explanation:** Weights are **learned parameters**, not hyperparameters.

---

### **9. Early stopping can be considered a:**  
**Options:** A) Loss function B) Feature selection method C) Hyperparameter tuning strategy D) Kernel trick  
‚úÖ **Answer:** C  
üìò **Explanation:** Early stopping is a tuning strategy to avoid overfitting.

---

### **10. In Random Search, what is randomly selected?**  
**Options:** A) Data samples B) Evaluation metric C) Hyperparameter combinations D) Model type  
‚úÖ **Answer:** C  
üìò **Explanation:** Random search picks hyperparameter sets at random.

---

## ‚úÖ Final Score: 8 / 10  
üîÅ **Review Q2 and Q10 for perfect clarity.**

---


## Shallow Learning Model vs. Deep Learning Model

# ü§ñ Shallow Learning vs Deep Learning ‚Äì MCQs + Revision

---

## üìò Summary

| Aspect                | Shallow Learning                         | Deep Learning                              |
|-----------------------|------------------------------------------|---------------------------------------------|
| **Layers**            | 1 or 2                                   | Many hidden layers (deep)                  |
| **Feature Engineering** | Manual                                  | Automatic (via layers)                      |
| **Examples**          | Logistic Regression, SVM, k-NN, Decision Trees | CNN, RNN, DNN, Transformers       |
| **Data Requirement**  | Low to moderate                          | High ‚Äî needs large labeled data            |
| **Interpretability**  | High                                     | Low (black-box nature)                     |
| **Training Speed**    | Faster                                   | Slower                                     |
| **Application**       | Tabular data, simple tasks               | Image, audio, text, complex tasks          |
| **Computational Need**| Low ‚Äî runs on CPU                        | High ‚Äî usually needs GPU                   |

---

## ‚úÖ MCQs with Answers and Explanations

---

### **1. Which of the following is a shallow learning algorithm?**  
**Options:** A) CNN B) SVM C) RNN D) Transformer  
‚úÖ **Answer:** B  
üìò **Explanation:** SVM is a classical shallow learning algorithm.

---

### **2. What is a major difference between shallow and deep learning?**  
**Options:** A) Shallow needs more data B) Deep uses fewer parameters  
C) Shallow models rely on manual feature extraction D) Deep models are easier to interpret  
‚úÖ **Answer:** C  
üìò **Explanation:** Shallow learning often requires hand-crafted features.

---

### **3. Which model is best for structured tabular data?**  
**Options:** A) DNN B) CNN C) Logistic Regression D) RNN  
‚úÖ **Answer:** C  
üìò **Explanation:** Logistic Regression performs well on structured/tabular datasets.

---

### **4. Which is NOT a deep learning model?**  
**Options:** A) Decision Tree B) CNN C) RNN D) Transformer  
‚úÖ **Answer:** A  
üìò **Explanation:** Decision Trees are shallow, not deep.

---

### **5. Which model is faster to train on small datasets?**  
**Options:** A) Deep Learning B) Shallow Learning C) Both same D) None  
‚úÖ **Answer:** B  
üìò **Explanation:** Shallow models train quickly and are ideal for small data.

---

### **6. Deep learning models require:**  
**Options:** A) Small datasets B) Minimal compute  
C) High interpretability D) Large labeled datasets  
‚úÖ **Answer:** D  
üìò **Explanation:** Deep learning thrives with massive labeled datasets.

---

### **7. Which model does automatic feature extraction?**  
**Options:** A) k-NN B) SVM C) DNN D) Linear Regression  
‚úÖ **Answer:** C  
üìò **Explanation:** Deep Neural Networks learn features from raw input automatically.

---

### **8. Deep learning clearly outperforms shallow learning in:**  
**Options:** A) Credit scoring B) Spam filtering C) Image recognition D) Linear regression  
‚úÖ **Answer:** C  
üìò **Explanation:** For images and unstructured data, deep models are superior.

---

### **9. One downside of deep learning is:**  
**Options:** A) Higher interpretability B) Less data needed  
C) Harder to train and interpret D) No hidden layers  
‚úÖ **Answer:** C  
üìò **Explanation:** Deep models are computationally and interpretively complex.

---

### **10. Which is true about shallow learning?**  
**Options:** A) Needs GPU B) Best for unstructured data  
C) Good for low-data environments D) Uses many hidden layers  
‚úÖ **Answer:** C  
üìò **Explanation:** Shallow models work well when data is limited.

---

## ‚úÖ Final Score: 10 / 10  
üéØ You‚Äôve mastered this topic! Consider reviewing:  
- CNNs vs DNNs  
- Transfer learning  
- Bias-variance tradeoff in deep models  

---


## Back Propagation

# üîÑ Backpropagation ‚Äì Deep Learning Revision

---

## üìò What is Backpropagation?

Backpropagation is the **core algorithm** used to train neural networks. It calculates how the weights should be updated to reduce error using **gradient descent**.

---

## üß† Key Concepts

- **Forward Pass:** Compute predictions with current weights.
- **Loss Function:** Quantify the difference between prediction and target.
- **Backward Pass:** Use **chain rule** to compute gradients of the loss.
- **Weight Update Rule:**  
  \[
  w = w - \eta \cdot \frac{\partial L}{\partial w}
  \]  
  - \( \eta \): Learning rate  
  - \( \frac{\partial L}{\partial w} \): Gradient of loss w.r.t. weights

---

## ‚ùó Common Issues

- **Vanishing Gradient:** Gradients become too small ‚Üí slow learning (e.g., sigmoid)
- **Exploding Gradient:** Gradients become too large ‚Üí unstable updates
- **Solutions:** Use **ReLU**, **batch normalization**, **gradient clipping**

---

## ‚úÖ MCQs with Answers and Explanations

---

### **1. What is the main purpose of backpropagation in neural networks?**  
**Options:**  
A) Increase accuracy  
B) Reduce the model size  
C) Compute gradients to update weights  
D) Select activation function  
‚úÖ **Answer:** C  
üìò **Explanation:** It calculates gradients of the loss to update weights.

---

### **2. Backpropagation is based on which mathematical concept?**  
**Options:**  
A) Linear algebra  
B) Chain rule of derivatives  
C) Matrix inversion  
D) Lagrange multipliers  
‚úÖ **Answer:** B  
üìò **Explanation:** Chain rule allows gradient flow through multiple layers.

---

### **3. Which of the following is updated during backpropagation?**  
**Options:**  
A) Input features  
B) Activation functions  
C) Weights and biases  
D) Number of layers  
‚úÖ **Answer:** C  
üìò **Explanation:** Only weights and biases are adjusted during training.

---

### **4. In backpropagation, the error is propagated:**  
**Options:**  
A) Forward from input to output  
B) Backward from output to input  
C) Only in output layer  
D) Randomly across layers  
‚úÖ **Answer:** B  
üìò **Explanation:** It flows backward to compute partial derivatives layer by layer.

---

### **5. Which function causes vanishing gradients most often?**  
**Options:**  
A) ReLU  
B) Sigmoid  
C) Softmax  
D) Linear  
‚úÖ **Answer:** B  
üìò **Explanation:** Sigmoid outputs very small gradients for large input magnitudes.

---

### **6. Which of the following is true about the learning rate in backpropagation?**  
**Options:**  
A) Higher is always better  
B) Controls step size of weight updates  
C) It doesn't affect convergence  
D) Only used in output layer  
‚úÖ **Answer:** B  
üìò **Explanation:** It defines how big a step we take in the weight update direction.

---

### **7. Which of the following reduces vanishing gradient issues?**  
**Options:**  
A) Sigmoid activation  
B) Tanh activation  
C) ReLU activation  
D) High learning rate  
‚úÖ **Answer:** C  
üìò **Explanation:** ReLU avoids gradient shrinkage for positive inputs.

---

### **8. During backpropagation, gradients are used to:**  
**Options:**  
A) Reset weights  
B) Visualize the network  
C) Update weights in the opposite direction of error  
D) Increase output probability  
‚úÖ **Answer:** C  
üìò **Explanation:** Gradient descent moves weights **against** the gradient to reduce error.

---

### **9. Backpropagation stops when:**  
**Options:**  
A) Model crashes  
B) Loss function increases  
C) Gradient becomes negative  
D) Convergence or early stopping criteria is met  
‚úÖ **Answer:** D  
üìò **Explanation:** Training halts when loss converges or early stopping conditions are met.

---

### **10. What helps backpropagation to efficiently train deep networks?**  
**Options:**  
A) Constant weights  
B) Chain rule  
C) Random updates  
D) Low input variance  
‚úÖ **Answer:** B  
üìò **Explanation:** Chain rule links partial derivatives layer by layer.

---

## ‚úÖ Final Score: 10 / 10  
üéØ You‚Äôve mastered backpropagation! You're fully exam-ready on this topic.

---

### Random state

What is random_state?
random_state is a seed value used to control the randomness of an algorithm ‚Äî to make the results reproducible.

üß† Why It Matters
Many ML functions involve randomness, such as:

Splitting data (train_test_split)

Initializing model weights

Shuffling data in cross-validation

Creating random forests or decision trees

By setting random_state, you fix the randomness, so you get the same result every time you run the code.

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
Every time you run this with **random_state=42**, you get exactly the same train/test split.

If you don't set it, results will vary each time you run the code.

## Deep Learning Libraries

# ü§ñ Deep Learning Libraries ‚Äì Revision Notes

---

## üìò Overview

Deep learning libraries provide the tools and building blocks to design, train, and deploy neural networks efficiently.

---

## üîπ 1. TensorFlow

- **Developer:** Google Brain  
- **Language:** Python (core in C++)  
- **High-level API:** `tf.keras`  
- **Strengths:**  
  - Production-ready and scalable  
  - Supports GPU/TPU  
  - Mobile and embedded deployment  
- **Use Cases:**  
  - Image classification  
  - NLP  
  - Speech recognition  

---

## üîπ 2. PyTorch

- **Developer:** Facebook AI Research  
- **Language:** Python (core in C++)  
- **Key Features:**  
  - Dynamic computation graph  
  - Easy debugging  
  - Flexible for research  
- **Use Cases:**  
  - Computer vision  
  - NLP  
  - Reinforcement learning  

---

## üîπ 3. Keras

- **Now part of TensorFlow as:** `tf.keras`  
- **Strengths:**  
  - Simple and user-friendly  
  - Best for beginners  
- **Use Cases:**  
  - Quick prototyping  
  - Educational projects  
  - Shallow-to-moderate networks  

---

## üîπ 4. MXNet

- **Developer:** Apache  
- **Supported by:** Amazon (AWS SageMaker)  
- **Strengths:**  
  - Scalable to multi-GPU and cloud environments  
  - Good for large-scale training  

---

## üîπ 5. JAX

- **Developer:** Google  
- **Strengths:**  
  - Combines NumPy with AutoDiff  
  - Fast performance  
- **Use Cases:**  
  - Scientific computing  
  - Experimental deep learning research  

---

## üîπ 6. CNTK (Microsoft Cognitive Toolkit)

- **Developer:** Microsoft  
- **Note:** Not widely used today  
- **Good for:** RNNs and speech-related models  

---

## üìä Quick Comparison Table

| Library    | Best For              | Backend     | Key Strength         |
|------------|-----------------------|-------------|----------------------|
| TensorFlow | Production, deployment| C++ / Python| Scalability          |
| PyTorch    | Research, flexibility | C++ / Python| Pythonic, dynamic    |
| Keras      | Beginners             | TensorFlow  | Simplicity           |
| MXNet      | Cloud apps (AWS)      | C++         | Parallel training    |
| JAX        | Speed + AutoDiff      | Python      | Scientific research  |

---

## üß≠ When to Use What?

| Goal                                  | Recommended Library |
|---------------------------------------|---------------------|
| Quick prototyping                     | Keras, PyTorch      |
| Large production deployment           | TensorFlow          |
| Research/experimentation              | PyTorch, JAX        |
| Working with AWS Cloud                | MXNet               |

---

# S22: Deep Learning

---
---
#


---
---
#


---
---

### Confusion matrix and Precision, accuracy and recall


## LSTM (Long short Term Memory Neural Network) 


# Topics that can have numerical questions asked:

1. GINI
2. Entropy