# S20: Introduction to Neural Network

---
## Perceptron

---
## Multi layer Perceptron

---
## Classical Activation function and when to use them

---
### Sigmoid Activation Function

---
### tanh Activation Function

---
### ReLU Activation Function

---
### Linear Activation Function

---
### When to use Softmax over sigmoid activation function?

---
## Loss Functions - Regression
  - Question on Loss functions (MAE, MSE, and RMSE)

## Loss function - Classification
- Cross Entropy

## Hyperparameter Tuning

## Shallow Learning Model

## Deep Learning Model

## Back Propagation

## Boston Dataset Example

### Random state

## Deep Learning Libraries

### Karas

### Karas with Tensorflow

### Pytorch

---
---

# S22: Deep Learning

---
---
#


---
---
#


---
---


### Loss Functions - Classification MCQ questions


### Hyperparameter Tuning


### Back propagation 


### Confusion matrix and Precision, accuracy and recall


  - Question on Back-propogation: training multilayer perceptrons
  - Dropout can be a exam question
  - Array of sigmoid - softmax will be a question in exam.

  - Multi Layer perceptron and its structure
  - Epoch, batch and mini batch

   - LSTM (Long short Term Memory Neural Network) 


# Topics that can have numerical questions asked:

1. GINI
2. Entropy